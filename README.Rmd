---
title: "termco"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  md_document:
    toc: true      
---

```{r, echo=FALSE, message=FALSE}
desc <- suppressWarnings(readLines("DESCRIPTION"))
regex <- "(^Version:\\s+)(\\d+\\.\\d+\\.\\d+)"
loc <- grep(regex, desc)
ver <- gsub(regex, "\\2", desc[loc])
verbadge <- sprintf('<a href="https://img.shields.io/badge/Version-%s-orange.svg"><img src="https://img.shields.io/badge/Version-%s-orange.svg" alt="Version"/></a></p>', ver, ver)
library(dplyr);library(termco);library(knitr)
````


```{r, echo=FALSE}
knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption"><b><em>',options$htmlcap,"</em></b></p>",sep="")
    }
    })
knitr::opts_knit$set(self.contained = TRUE, cache = FALSE)
knitr::opts_chunk$set(fig.path = "inst/figure/")
```

[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)
[![Build Status](https://travis-ci.org/trinker/termco.svg?branch=master)](https://travis-ci.org/trinker/termco)
[![Coverage Status](https://coveralls.io/repos/trinker/termco/badge.svg?branch=master)](https://coveralls.io/r/trinker/termco?branch=master) 
[![DOI](https://zenodo.org/badge/5398/trinker/termco.svg)](https://zenodo.org/badge/latestdoi/5398/trinker/termco)`r verbadge`

<img src="inst/termco_logo/r_termco.png" width="200" alt="qdapRegex Logo">  

**termco** is A small suite of functions used to count and find terms and substrings in strings.  The tools can be used to build an expert rules, regular expression based text classification model.  The package wraps the [**data.table**](https://cran.r-project.org/package=data.table) and [**stringi**](https://cran.r-project.org/package=stringi) packages to create fast data frame counts of regular expression terms and substrings.   

# Installation

To download the development version of **termco**:

Download the [zip ball](https://github.com/trinker/termco/zipball/master) or [tar ball](https://github.com/trinker/termco/tarball/master), decompress and run `R CMD INSTALL` on it, or use the **pacman** package to install the development version:

```r
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("trinker/termco")
```

# Contact

You are welcome to:
* submit suggestions and bug-reports at: <https://github.com/trinker/termco/issues>
* send a pull request on: <https://github.com/trinker/termco/>
* compose a friendly e-mail to: <tyler.rinker@gmail.com>

## Examples

The following examples demonstrate some of the functionality of **termco**.

```{r, echo=FALSE, message=FALSE}
library(termco); library(qdapRegex)
```

```{r}
data(presidential_debates_2012)

discoure_markers <- list(
    response_cries = c("oh", "ah", "aha", "ouch", "yuk"),
    back_channels = c("uh[- ]huh", "uhuh", "yeah"),
    summons = "hey",
    justification = "because"
)

with(presidential_debates_2012, term_count(dialogue, list(person, time), discoure_markers))
```


### Print Method

```{r}
print(markers, pretty = FALSE)
print(markers, zero.replace = "_")
```

### Plot Method

```{r}
plot(markers)
plot(markers, labels=TRUE)
plot_ca(markers, FALSE)
```

## Building an Expert Rules, Regex Classifier Model

Machine learning models of classification are great when you have known tags to train with because the model scales.  Qualitative, expert based human coding is terrific for when you have no tagged data. However, when you have a larger, untagged data set the machine learning approaches have no outcome to learn from and the data is too large to classify by hand.  One solution is to use a expert rules, regular expression approach that is somewhere between machine learning and hand coding.  This is one solution for tagging larger, untagged data sets.

This example section highlights the types of function combinations and order for a typical expert rules classification.  This task typically involves the combined use of available literature, close examinations of term usage within text, and researcher experience.  Building a classifier model requires the researcher to build a list of regular expressions that map to a category or tag.  Below I outline minimal work flow for classification.

### Load the Tools/Data

```{r}
library(dplyr); library(ggplot2)
data(presidential_debates_2012)
```

### View Most Used Words

A common task in building a model is to understand the most frequent words while excluding less information rich function words.  The `frequnt_terms` function produces an ordered data frame of counts.  The researcher can exclude stop words and limit the terms to contain n characters between set thresholds.  The output is ordered by most to least frequent n terms but can be rearranged alphabetically.

```{r}
presidential_debates_2012 %>%
    with(., frequent_terms(dialogue))
presidential_debates_2012 %>%
    with(., frequent_terms(dialogue, 40)) %>%
    plot()
```

### Building the Model

To build a model the researcher created a named list of regular expressions that map to a category/tag.  This is fed to the `term_count` function.  `term_count` allows for aggregation by grouping variables but for building the model we usually want to get observation level counts.  Set `grouping.var = TRUE` to generate an `id` column of 1 through number of observation which gives the researcher the observation level counts.

```{r}
discoure_markers <- list(
    response_cries = c("oh", "ah", "aha", "ouch", "yuk"),
    back_channels = c("uh[- ]huh", "uhuh", "yeah"),
    summons = "hey",
    justification = "because"
)

model <- presidential_debates_2012 %>%
    with(term_count(dialogue, grouping.var = TRUE, discoure_markers))

model
```


### Testing the Model

In building a classifier the researcher is typically concerned with coverage, discrimination, and accuracy.  The first two are easier to obtain while accuracy is not possible to compute without a comparison sample of expertly tagged data.

We want out model to be giving tags to as much of the text elements as possible.  The `coverage` function can provide an understanding of what percent of the data is tagged. Out model has relatively low coverage, indicating the regular expression model needs to be improved.

```{r}
model %>%
    coverage()
```

Understanding how well our model discriminates is important as well.  We want the model to cover as close to 100% of the data as possible, but likely want fewer tags assigned to each element.  If the model is tagging many tags to each element it is not able to discriminate well.  The `as_terms` + `plot_freq` function provides a visual representation of the model's ability to discriminate.  The output is a bar plot showing the distribution of the number of tags at the element level.  The goal is to have a larger density at $1$ tag.  Note that the plot also gives a view of coverage, as the zero bar shows the frequency of elements that could not be tagged.  Our model has a larger distribution of $1$ tag compared to the $> 1$ tag distributions, though the coverage is very poor.  As the number of tags increases the ability of the model to discriminate typically lessens.  There is often a trade off between model coverage and discrimination.

```{r}
model %>%
    as_terms() %>%
    plot_freq(size=3) + xlab("Number of Tags")
```

We may also want to see the distribution of the tags as well.  The combination of `as_terms` + `plot_counts` gives the distribution of the tags.  In our model the majority of tags are applied to the **summons** category. 

```{r}
model %>%
    as_terms() %>%
    plot_counts() + xlab("Tags")
```

### Improving the Model

The model does not have very good coverage.  To improve this the researcher will want to look at the data with no coverage to try to build additional regular expressions and categories.  This requires understanding language, noticing additional features of the data with no coverage that may map to categories, and building regular expressions to model these features.  This section will outline some of the tools that can be used to detect features and build regular expressions to model these language features.

We first want to view the untagged data.  The `uncovered function provides a logical vector that can be used to exctract the text with no tags.

```{r}
untagged <- presidential_debates_2012 %>%
    select(dialogue) %>%
    {unlist(., use.names=FALSE)[uncovered(model)]}

head(untagged)
```

The `frequent_terms` function can be used again to understand common features of the untagged data.

```{r}
untagged %>%
    frequent_terms()
```

We may see a common term such as the word *right* and want to see what other terms collocate with it.  Using a regular expression that searches for multiple terms can improve a model's accuracy and ability to discriminate.  Using `search_term` in combination with `frequent_terms` can be a powerful way to see which words tend to collocate.  Here I pass a regex for *right* (`\\bright`) to `search_term`.  This pulls up the text that contains this term.  I then use `frequent_terms` to see what words frequently occur with the word *right*.  We notice the word *people* tends to occur with *right*.

```{r}
untagged %>%
    search_term("\\bright") %>%
    frequent_terms(10)
```

This is an exploratory act.  Finding the right combination of features that occur together requires lots of recursive noticing, trialling, testing, reading, interpreting, and deciding.  In the example below I noticed that terms *people* and *course* appear with the term *right*.  I use a grouped-or expression with `colo` to build a regular expression that will search for any text elements that contain these two terms anywhere.  `colo` is more powerful than initially shown here; I demonstrate further functionality below.

```{r}
colo("\\bright", "(people|course)")
```

This is extremely powerful when used inside of `search_term` as the text containing this regular expression will be returned along with the coverage proportion on the uncovered data.

```{r}
search_term(untagged, colo("\\bright", "(people|course)"))
```

We notice right away that the phrase *right course* appears often.  I create a search with just this expression. 
Note that the decision to include a regular expression in the model is up to the researcher.  We must guard against overfitting the model, making it not transferable to new, similar contexts.

```{r}
search_term(untagged, "right course")
```

The word *jobs* also seems important.  Again, I use the `search_term` + `frequent_terms` combo to extract words collocating with *jobs*.

```{r}
search_term(untagged, "jobs") %>%
    frequent_terms(10)
```

As stated above, `colo` is a powerful search tool as it can take multiple regular expressions as well as allowing for multiple negations (i.e., find x but not if y).  To include multiple negations use a grouped-or regex as shown below.

```{r}
## Where do `jobs` and `create` collocate?
search_term(untagged, colo("jobs", "create")) 
## Where do `jobs`, `create`,  and the word `not` collocate?
search_term(untagged, colo("jobs", "create", "(not|'nt)")) 
## Where do `jobs` and`create` collocate without a `not` word?
search_term(untagged, colo("jobs", "create", not = "(not|'nt)")) 
```

Here is one more example with `colo` for the words *jobs* and *overseas*.  The user may want to quickly test and then transfer the regex created by `colo` to the regular expression list.  By setting `options(termco.copy2clip = TRUE)` the user globally sets `colo` to use the **clipr** package to copy the regex to the clipboard for better work flow.

```{r}
search_term(untagged, colo("jobs", "overseas")) 
```

The researcher uses an iterative process to continue to build the regular expression list.  The `term_count` function builds the matrix of counts to further test the model.  The use of (a) `coverage`, (b) `as_terms` + `plot_counts`, and &#40;c) `as_terms` + `freq_counts` will allow for continued testing of model functioning.  

### Categorizing/Tagging

The `classify` function enables the researcher to apply n tags to each text element.  Depending on the text and the regular expression list's ability, multiple tags may be applied to a text.  The `n` argument allows the maximum number of tags to be set though the function does not guarantee this many (or any) tags will be assigned.

Here I show the `head` of the returned vector (if `n` > 1 a `list` may be returned) as well as a `table` and plot of the counts.

```{r}
classify(model) %>%
    head()
classify(model) %>%
    unlist() %>%
    table()
classify(model) %>%
    unlist() %>%
    plot_counts() + xlab("Tags")
```


### Accuracy

The user may be interested in testing the accuracy of the model against a known, human coded sample.  The `accuracy` function allows the researcher to test a model's accuracy.  In the example below I randomly generate "known human coded tagged" vector.  Obviously, this is for demonstration purposes.  The model outputs a pretty printing of a list. The printing contains: 

| Printed    |  Description                                          | Accessed |
|--------------------|-----------------------------------------------|----------|
|  **N**     |The number of text elements (`N`)                      |          |
| **Exact**  | Perfect one to one correspondence between whole vectors in `tagged` & `known` | `$exact` |
| **Ordered** | Proportion of the elements in `tagged` matching `known` exactly; order matters | `$ordered` |
| **Adjusted** | An adjusted mean score of **Ordered** and **unordered** | `$adjusted` |
| **Unordered** | Proportion of the elements in `tagged` matching `known` regardless of order | `$unordered` |

Note that if classify is constrained to `n = 1` then all scores will be identical.  If a larger, known tagging is available the user may want to strongly consider machine learning models (see: [**RTextTools**](https://cran.r-project.org/package=RTextTools)).

This minimal example will provide insight into the way the accuracy scores behave:

```{r}
known <- list(1:3, 3, NA, 4:5, 2:4, 5, integer(0))
tagged <- list(1:3, 3, 4, 5:4, c(2, 4:3), 5, integer(0))
accuracy(tagged, known)
```

Below we create fake "known" tags to test `accuracy` with real data (though the comparison is fabricated).

```{r}
mod1 <- presidential_debates_2012 %>%
    with(., term_count(dialogue, TRUE, discoure_markers)) %>%
    classify()

fake_known <- mod1
set.seed(1)
fake_known[sample(1:length(fake_known), 300)] <- "random noise"

accuracy(mod1, fake_known)
```

In this model we allow for `n = 3` tags to be assigned in the classification.  This enables the potential for a (in this case *slightly*) better a **Adjusted** value.  The adjusted value upweights based on correctly applying a tag regardless of the position of that tag (`classify` gives higher preference to tags that have a higher term count for that text element; order for ties is broken randomly).

```{r}
mod2 <- presidential_debates_2012 %>%
    with(., term_count(dialogue, TRUE, discoure_markers)) %>%
    classify(n = 3)

fake_known2 <- mod2
set.seed(30)
fake_known2[sample(1:length(fake_known2), 500)] <- c("random noise", "back_channels")

accuracy(mod2, fake_known2)
```

These examples give guadance on how to use the tools in the **termco** package to build an expert rules, regular expression text classification model.


